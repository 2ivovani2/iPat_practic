# -*- coding: utf-8 -*-
"""ROST_last_gen.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16RXc30fXJBOT-VIa4hhKfyRL3gD5ap_z
"""

from google.colab import drive
drive.mount('/content/drive')

!python -m pip install pymystem3
!python -m pip install emoji
!python -m pip install pandarallel
!python -m pip install sumy

import warnings, string, emoji
warnings.filterwarnings("ignore")

import pandas as pd
import numpy as np
import time
import matplotlib.pyplot as plt

import nltk, string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from pymystem3 import Mystem

from pandarallel import pandarallel
pandarallel.initialize(progress_bar=True, nb_workers=8)

nltk.download('punkt')
nltk.download('stopwords')

messages = pd.read_csv("/content/drive/MyDrive/meds_public_messages.csv", header=None)

heads = ['id', 'text', 'sent', 'deadline', "is_doctor_message", 'contract_id', 'created_at', 'updated_at',
    "is_answered", "warning_sent", "deadline_sent", 'answered', "is_read", 'current_doctor_id',
    "notification_sent", 'is_filtered', 'is_auto', 'is_agent', 'agent_id', 'action_link','action_name',
    'only_doctor', 'is_urgent', 'forward_to_doctor', 'action_onetime', 'action_used', 'action_deadline',
    'only_patient', 'action_big', 'author_id', 'author_role', 'is_mailing', 'registrator_id', 'is_declined',
    'reply_to_id', 'action_type', 'is_warning']

messages.columns = heads
messages.sample(5)

patient_messages = messages[(messages.is_doctor_message == False) & (messages.text != "Голосовое сообщение")][["id", "text"]]
patient_messages.dropna(inplace=True)
patient_messages.sample(5)

"""# Векторизация с помощью Navec"""

STOP_WORDS = set(stopwords.words("russian"))
HELLO_WORDS = set(['Доброе утро', "Здравствуйте", "Добрый день", "Добрый вечер"])
LEMMATIZER = Mystem()

def remove_punct(message: str) -> str:
    # удаляем знаки пунктуации
    message = message.translate(str.maketrans(' ', ' ', string.punctuation + "\n-«»№1234567890"))

    # удаляем лишние пробелы  и переведем все в нижний регистр
    message = " ".join(filter(lambda word: word != " ", message.split())).lower()
    return message

def remove_emoji(text: str) -> str:
    # удаляем емоджи
    return emoji.replace_emoji(text, replace="")

def remove_stopwords(message: str) -> str:
    # удаляем стоп слова
    message = ' '.join([word for word in word_tokenize(message) if word not in STOP_WORDS.union(HELLO_WORDS)])
    return message

def lemmatize_msg(message: str) -> str:
    # лемматизируем сообщения
    message = " ".join([LEMMATIZER.lemmatize(w)[0] for w in word_tokenize(message)])
    return message

def preproc(patient_messages):
    patient_messages.dropna(inplace=True)
    patient_messages.text = patient_messages.text.parallel_apply(remove_punct).parallel_apply(remove_emoji)
    patient_messages.text = patient_messages.text.parallel_apply(remove_stopwords)
    patient_messages = patient_messages[patient_messages.text != ""]
    patient_messages.text = patient_messages.text.parallel_apply(lemmatize_msg)
    return patient_messages

patient_messages_preproc = preproc(patient_messages)

!wget https://storage.yandexcloud.net/natasha-navec/packs/navec_hudlit_v1_12B_500K_300d_100q.tar
!pip install navec

from navec import Navec

path = 'navec_hudlit_v1_12B_500K_300d_100q.tar'
navec = Navec.load(path)

def text_2_vec(text):
    words = text.split()
    embeddings = []

    for word in words:
        try:
            vector = navec[word]
        except:
            vector = np.zeros(300)

        embeddings.append(vector)

    return np.array(embeddings).mean(axis=0)

df = pd.DataFrame({
  "id": patient_messages_preproc.id,
  "message": patient_messages_preproc.text.values,
  "message_emb":patient_messages_preproc.text.parallel_apply(text_2_vec)
})

emb_matrix = np.array(df.message_emb.to_list())

from joblib import Parallel, delayed
from sklearn.metrics import silhouette_samples
from tqdm import tqdm

n_jobs = 8

def parallel_silhouette_samples(X, labels, metric='cosine', n_jobs=1):
    n_samples = X.shape[0]
    ranges = np.array_split(range(n_samples), n_jobs)

    silhouette_samples_pieces = Parallel(n_jobs=n_jobs)(
        delayed(silhouette_samples)(X[r], labels[r], metric=metric) for r in ranges
    )
    return np.concatenate(silhouette_samples_pieces).mean()

from sklearn.metrics import silhouette_score
from sklearn.cluster import MiniBatchKMeans

silhouette_scores = []

for k in tqdm(range(5, 50, 5)):
    kmeans = MiniBatchKMeans(n_clusters=k, batch_size=1000, random_state=42).fit(emb_matrix)
    score = parallel_silhouette_samples(emb_matrix, kmeans.labels_, metric="cosine")
    silhouette_scores.append(score)

plt.plot(range(5, 50, 5), silhouette_scores, 'bx-')
plt.xlabel('Кол-во кластеров')
plt.ylabel('Силуэтное расстояние')
plt.title('Силуэтный метод для эмбеддингов Navec')
plt.show()

from sklearn.decomposition import PCA
from sklearn.cluster import KMeans

optimal_k = 15

pca = PCA(50)
data_pca = pca.fit_transform(emb_matrix)

kmeans = KMeans(n_clusters=optimal_k, random_state=42).fit(data_pca)

df["cluster"] = kmeans.labels_
df.head()

df[df.cluster == 0].sample(5)

"""# Векторизация с помощью transformers"""

import torch
from transformers import AutoTokenizer, AutoModel

# Загрузка предобученной модели и токенизатора
model_name = "cointegrated/rubert-tiny2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

def get_embeddings(texts):
    # Токенизация входных текстов
    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")

    # Получение эмбеддингов с помощью модели
    with torch.no_grad():
        outputs = model(**inputs)

    # Emбеддинги находятся в outputs.last_hidden_state
    # Мы берем эмбеддинг [CLS] токена, который соответствует первому токену
    embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
    return embeddings[0]

patient_messages["embedding"] = patient_messages["text"].parallel_apply(get_embeddings)

emb_matrix = np.array(patient_messages.embedding.to_list())

# patient_messages_sample = patient_messages.sample(30000)

from sklearn.metrics import silhouette_score
from sklearn.cluster import MiniBatchKMeans

silhouette_scores = []

for k in tqdm(range(5, 50, 5)):
    kmeans = MiniBatchKMeans(n_clusters=k, batch_size=1000, random_state=42).fit(emb_matrix)
    score = parallel_silhouette_samples(emb_matrix, kmeans.labels_, metric="cosine")
    silhouette_scores.append(score)

plt.plot(range(5, 50, 5), silhouette_scores, 'bx-')
plt.xlabel('Кол-во кластеров')
plt.ylabel('Силуэтное расстояние')
plt.title('Силуэтный метод для transformers')
plt.show()

optimal_k = 15

pca = PCA(50)
data_pca = pca.fit_transform(emb_matrix)

kmeans = KMeans(n_clusters=optimal_k, random_state=42).fit(data_pca)

patient_messages["cluster"] = kmeans.labels_
patient_messages.head()

patient_messages[patient_messages.cluster == 10].sample(5)

np.array((sorted(patient_messages.cluster.unique())))